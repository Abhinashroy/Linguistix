{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Model being implemented: CNN (on the raw extracted dataset)"
      ],
      "metadata": {
        "id": "RnZYZiLaco92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "-1J2sJt-fTXc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Implementing CNN from scratch using NumPy (CPU-only)\")\n",
        "\n",
        "# Load dataset\n",
        "X_features = np.load(\"X_features.npy\")\n",
        "y_labels = np.load(\"y_labels.npy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c9bL8eljVu5",
        "outputId": "de5b5ad7-f731-41ed-c8e2-7c949a6bb40a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Implementing CNN from scratch using NumPy (CPU-only)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "T81mIJYhdC6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_features)\n",
        "\n",
        "# Reshape to 2D \"images\"\n",
        "height = int(np.sqrt(X_scaled.shape[1]))\n",
        "width = int(np.ceil(X_scaled.shape[1] / height))\n",
        "padding = height * width - X_scaled.shape[1]\n",
        "X_padded = np.pad(X_scaled, ((0, 0), (0, padding)), mode='constant')\n",
        "X_reshaped = X_padded.reshape(-1, 1, height, width)  # (N, C, H, W)\n",
        "\n",
        "# Train-test split (using raw integer labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_reshaped, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
        ")"
      ],
      "metadata": {
        "id": "wYuWLYoVjaWq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for CNN\n",
        "class ScratchCNN:\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.params = {}\n",
        "        # Conv Layer 1: 3x3 kernel, 32 filters\n",
        "        self.params['W1'] = np.random.randn(32, 1, 3, 3) * 0.1\n",
        "        self.params['b1'] = np.zeros(32)\n",
        "        # Conv Layer 2: 3x3 kernel, 64 filters\n",
        "        self.params['W2'] = np.random.randn(64, 32, 3, 3) * 0.1\n",
        "        self.params['b2'] = np.zeros(64)\n",
        "        # FC Layer\n",
        "        flattened_size = 64 * (height//4) * (width//4)\n",
        "        self.params['W3'] = np.random.randn(flattened_size, 256) * 0.1\n",
        "        self.params['b3'] = np.zeros(256)\n",
        "        # Output Layer\n",
        "        self.params['W4'] = np.random.randn(256, num_classes) * 0.1\n",
        "        self.params['b4'] = np.zeros(num_classes)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv1 -> ReLU -> MaxPool\n",
        "        self.cache = {}\n",
        "        self.cache['Z1'] = self._conv2d(x, self.params['W1'], self.params['b1'])\n",
        "        self.cache['A1'] = self.relu(self.cache['Z1'])\n",
        "        self.cache['P1'] = self._maxpool2d(self.cache['A1'])\n",
        "\n",
        "        # Conv2 -> ReLU -> MaxPool\n",
        "        self.cache['Z2'] = self._conv2d(self.cache['P1'], self.params['W2'], self.params['b2'])\n",
        "        self.cache['A2'] = self.relu(self.cache['Z2'])\n",
        "        self.cache['P2'] = self._maxpool2d(self.cache['A2'])\n",
        "\n",
        "        # Flatten -> FC -> Output\n",
        "        self.cache['F'] = self.cache['P2'].reshape(self.cache['P2'].shape[0], -1)\n",
        "        self.cache['Z3'] = np.dot(self.cache['F'], self.params['W3']) + self.params['b3']\n",
        "        self.cache['A3'] = self.relu(self.cache['Z3'])\n",
        "        self.cache['Z4'] = np.dot(self.cache['A3'], self.params['W4']) + self.params['b4']\n",
        "        self.cache['A4'] = self.softmax(self.cache['Z4'])\n",
        "        return self.cache['A4']\n",
        "\n",
        "    def _conv2d(self, x, W, b, stride=1, pad=1):\n",
        "        N, C, H, W_in = x.shape\n",
        "        F, _, HH, WW = W.shape  # F = number of filters\n",
        "\n",
        "        # Add padding\n",
        "        x_pad = np.pad(x, ((0,0), (0,0), (pad,pad), (pad,pad)), mode='constant')\n",
        "\n",
        "        # Output dimensions\n",
        "        H_out = (H + 2*pad - HH) // stride + 1\n",
        "        W_out = (W_in + 2*pad - WW) // stride + 1\n",
        "\n",
        "        out = np.zeros((N, F, H_out, W_out))\n",
        "\n",
        "        for n in range(N):  # For each sample in batch\n",
        "            for f in range(F):  # For each filter\n",
        "                for i in range(H_out):  # Slide vertically\n",
        "                    for j in range(W_out):  # Slide horizontally\n",
        "                        ii, jj = i*stride, j*stride\n",
        "                        # Element-wise multiplication and sum\n",
        "                        out[n,f,i,j] = np.sum(\n",
        "                            x_pad[n, :, ii:ii+HH, jj:jj+WW] * W[f]\n",
        "                        ) + b[f]\n",
        "        return out\n",
        "\n",
        "\n",
        "    def _maxpool2d(self, x, pool_size=2, stride=2):\n",
        "      N, C, H, W = x.shape\n",
        "      H_out = (H - pool_size) // stride + 1\n",
        "      W_out = (W - pool_size) // stride + 1\n",
        "\n",
        "      out = np.zeros((N, C, H_out, W_out))\n",
        "\n",
        "      for n in range(N):\n",
        "          for c in range(C):\n",
        "              for i in range(H_out):\n",
        "                  for j in range(W_out):\n",
        "                      ii, jj = i*stride, j*stride\n",
        "                      out[n,c,i,j] = np.max(\n",
        "                          x[n,c, ii:ii+pool_size, jj:jj+pool_size]\n",
        "                      )\n",
        "      return out\n",
        "\n",
        "    def compute_loss(self, outputs, y_true):\n",
        "        # Simplified loss using raw labels\n",
        "        correct_probs = outputs[np.arange(len(y_true)), y_true]\n",
        "        return -np.mean(np.log(correct_probs + 1e-10))  # Avoid log(0)\n",
        "\n",
        "    def backward(self, x, y_true, lr=0.001):\n",
        "        m = len(y_true)  # Batch size\n",
        "        grads = {key: np.zeros_like(val) for key, val in self.params.items()}  # Initialize all gradients\n",
        "\n",
        "        # Output layer gradient\n",
        "        dZ4 = self.cache['A4'].copy()\n",
        "        dZ4[np.arange(m), y_true] -= 1\n",
        "        dZ4 /= m\n",
        "\n",
        "        # FC layer gradients\n",
        "        grads['W4'] = np.dot(self.cache['A3'].T, dZ4)\n",
        "        grads['b4'] = np.sum(dZ4, axis=0)\n",
        "\n",
        "        dA3 = np.dot(dZ4, self.params['W4'].T)\n",
        "        dZ3 = dA3 * (self.cache['A3'] > 0)  # ReLU derivative\n",
        "\n",
        "        # Hidden layer gradients\n",
        "        grads['W3'] = np.dot(self.cache['F'].T, dZ3)\n",
        "        grads['b3'] = np.sum(dZ3, axis=0)\n",
        "\n",
        "        # Reshape for conv backprop\n",
        "        dF = np.dot(dZ3, self.params['W3'].T)\n",
        "        dP2 = dF.reshape(self.cache['P2'].shape)\n",
        "\n",
        "        # MaxPool2 backward\n",
        "        dA2 = np.zeros_like(self.cache['A2'])\n",
        "        for n in range(dP2.shape[0]):\n",
        "            for c in range(dP2.shape[1]):\n",
        "                for i in range(dP2.shape[2]):\n",
        "                    for j in range(dP2.shape[3]):\n",
        "                        ii, jj = i*2, j*2\n",
        "                        window = self.cache['A2'][n,c,ii:ii+2,jj:jj+2]\n",
        "                        mask = (window == np.max(window))\n",
        "                        dA2[n,c,ii:ii+2,jj:jj+2] = mask * dP2[n,c,i,j]\n",
        "\n",
        "        # Conv2 backward\n",
        "        dZ2 = dA2 * (self.cache['Z2'] > 0)\n",
        "\n",
        "        # Calculate gradients for W2 and b2\n",
        "        for n in range(x.shape[0]):\n",
        "            for f in range(self.params['W2'].shape[0]):\n",
        "                for c in range(self.params['W2'].shape[1]):\n",
        "                    for i in range(dZ2.shape[2]):\n",
        "                        for j in range(dZ2.shape[3]):\n",
        "                            ii, jj = i, j  # No stride in gradient calculation\n",
        "                            grads['W2'][f,c] += np.sum(\n",
        "                                self.cache['P1'][n,c,ii:ii+3,jj:jj+3] * dZ2[n,f,i,j]\n",
        "                            )\n",
        "                grads['b2'][f] += np.sum(dZ2[n,f])\n",
        "\n",
        "        # Backprop through Conv1\n",
        "        dP1 = np.zeros_like(self.cache['P1'])  # Shape: (N, C, H, W)\n",
        "        for n in range(x.shape[0]):  # Iterate over batch\n",
        "            for c in range(self.params['W2'].shape[1]):  # Loop over input channels (not filters!)\n",
        "                for f in range(self.params['W2'].shape[0]):  # Loop over filters\n",
        "                    kernel = np.rot90(self.params['W2'][f, c], 2)  # Rotate the filter\n",
        "\n",
        "                    # Add padding to dZ2 (same as Conv layer)\n",
        "                    pad = 1  # Because Conv used a 3x3 filter with stride 1\n",
        "                    dZ2_padded = np.pad(dZ2[n, f], ((pad, pad), (pad, pad)), mode='constant')\n",
        "\n",
        "                    # Convolve manually\n",
        "                    for i in range(dP1.shape[2]):  # Loop over height\n",
        "                        for j in range(dP1.shape[3]):  # Loop over width\n",
        "                            dP1[n, c, i, j] += np.sum(dZ2_padded[i:i+3, j:j+3] * kernel)  # Accumulate over filters\n",
        "\n",
        "\n",
        "        # MaxPool1 backward\n",
        "        dA1 = np.zeros_like(self.cache['A1'])\n",
        "        for n in range(dP1.shape[0]):\n",
        "            for c in range(dP1.shape[1]):\n",
        "                for i in range(dP1.shape[2]):\n",
        "                    for j in range(dP1.shape[3]):\n",
        "                        ii, jj = i*2, j*2\n",
        "                        window = self.cache['A1'][n,c,ii:ii+2,jj:jj+2]\n",
        "                        mask = (window == np.max(window))\n",
        "                        dA1[n,c,ii:ii+2,jj:jj+2] = mask * dP1[n,c,i,j]\n",
        "\n",
        "        # Conv1 backward\n",
        "        dZ1 = dA1 * (self.cache['Z1'] > 0)\n",
        "\n",
        "        # Calculating gradients for W1 and b1\n",
        "        for n in range(x.shape[0]):\n",
        "            for f in range(self.params['W1'].shape[0]):\n",
        "                for c in range(self.params['W1'].shape[1]):\n",
        "                    for i in range(dZ1.shape[2]):\n",
        "                        for j in range(dZ1.shape[3]):\n",
        "                            ii, jj = i, j  # No stride in gradient calculation\n",
        "                            grads['W1'][f,c] += np.sum(\n",
        "                                x[n,c,ii:ii+3,jj:jj+3] * dZ1[n,f,i,j]\n",
        "                            )\n",
        "                grads['b1'][f] += np.sum(dZ1[n,f])\n",
        "\n",
        "        # Normalising gradients by batch size\n",
        "        for param in grads:\n",
        "            grads[param] /= m\n",
        "\n",
        "        # Updating parameters\n",
        "        for param in self.params:\n",
        "            self.params[param] -= lr * grads[param]"
      ],
      "metadata": {
        "id": "NT_mnAz7eGPh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise and train\n",
        "num_classes = len(np.unique(y_labels))\n",
        "cnn = ScratchCNN(input_shape=(1, height, width), num_classes=num_classes)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = cnn.forward(batch_X)\n",
        "\n",
        "        # Loss calculation\n",
        "        loss = cnn.compute_loss(outputs, batch_y)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        # Backward pass\n",
        "        cnn.backward(batch_X, batch_y, lr=0.001)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "    losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "38OuxZozlNkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "test_outputs = cnn.forward(X_test)\n",
        "predicted_classes = np.argmax(test_outputs, axis=1)\n",
        "accuracy = accuracy_score(y_test, predicted_classes)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot training curve\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Progress\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lPXzDIKulJ_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5gZE_41nznp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}