{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS5hxTEeGubh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.load(\"X_features.npy\")  # MFCC features\n",
        "y = np.load(\"y_labels.npy\")  # Labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA Functions\n",
        "def compute_class_means(X, y):\n",
        "    class_labels = np.unique(y)\n",
        "    means = {label: np.mean(X[y == label], axis=0) for label in class_labels}\n",
        "    return means\n",
        "\n",
        "def compute_within_class_scatter(X, y, means):\n",
        "    n_features = X.shape[1]\n",
        "    Sw = np.zeros((n_features, n_features))\n",
        "    for label, mean in means.items():\n",
        "        class_scatter = np.dot((X[y == label] - mean).T, (X[y == label] - mean))\n",
        "        Sw += class_scatter\n",
        "    return Sw\n",
        "\n",
        "def compute_between_class_scatter(X, y, means):\n",
        "    overall_mean = np.mean(X, axis=0)\n",
        "    n_features = X.shape[1]\n",
        "    Sb = np.zeros((n_features, n_features))\n",
        "    for label, mean in means.items():\n",
        "        n_samples = X[y == label].shape[0]\n",
        "        mean_diff = (mean - overall_mean).reshape(-1, 1)\n",
        "        Sb += n_samples * np.dot(mean_diff, mean_diff.T)\n",
        "    return Sb\n",
        "\n",
        "def lda(X, y, num_components):\n",
        "    means = compute_class_means(X, y)\n",
        "    Sw = compute_within_class_scatter(X, y, means)\n",
        "    Sb = compute_between_class_scatter(X, y, means)\n",
        "\n",
        "    eigvals, eigvecs = np.linalg.eig(np.linalg.pinv(Sw).dot(Sb))\n",
        "    eigvals = eigvals.real  # Keep only the real parts\n",
        "    eigvecs = eigvecs.real  # Keep only the real parts\n",
        "\n",
        "    sorted_indices = np.argsort(eigvals)[::-1]\n",
        "    eigvecs = eigvecs[:, sorted_indices]\n",
        "    return eigvecs[:, :num_components]\n",
        "\n",
        "def project_data(X, eigenvectors):\n",
        "    return np.dot(X, eigenvectors)"
      ],
      "metadata": {
        "id": "C_1Mun92Q_m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, C=1.0, kernel='linear', tol=1e-3, max_iter=1000):\n",
        "        self.C = C  # Regularization parameter\n",
        "        self.kernel = self._get_kernel(kernel)\n",
        "        self.tol = tol  # Tolerance for stopping criteria\n",
        "        self.max_iter = max_iter  # Max iterations for SMO\n",
        "        self.alpha = None  # Lagrange multipliers\n",
        "        self.b = 0  # Bias term\n",
        "        self.w = None  # Weight vector (for linear kernel)\n",
        "        self.support_vectors = None  # Store support vectors\n",
        "\n",
        "    def _get_kernel(self, kernel_type):\n",
        "        if kernel_type == 'linear':\n",
        "            return lambda x, y: np.dot(x, y.T)\n",
        "        elif kernel_type == 'rbf':\n",
        "            return lambda x, y, gamma=0.5: np.exp(-gamma * np.linalg.norm(x - y) ** 2)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported kernel. Use 'linear' or 'rbf'.\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = np.where(y == 0, -1, 1)  # Convert to {-1,1} labels\n",
        "        self.alpha = np.zeros(n_samples)\n",
        "        self.b = 0\n",
        "\n",
        "        # Precompute the kernel matrix\n",
        "        K = np.zeros((n_samples, n_samples))\n",
        "        for i in range(n_samples):\n",
        "            for j in range(n_samples):\n",
        "                K[i, j] = self.kernel(X[i], X[j])\n",
        "\n",
        "        # SMO Algorithm\n",
        "        for _ in range(self.max_iter):\n",
        "            alpha_prev = np.copy(self.alpha)\n",
        "\n",
        "            for i in range(n_samples):\n",
        "                Ei = self._compute_error(X, y, K, i)\n",
        "                if (y[i] * Ei < -self.tol and self.alpha[i] < self.C) or (y[i] * Ei > self.tol and self.alpha[i] > 0):\n",
        "                    j = np.random.choice([k for k in range(n_samples) if k != i])  # Select random j != i\n",
        "                    Ej = self._compute_error(X, y, K, j)\n",
        "\n",
        "                    alpha_i_old, alpha_j_old = self.alpha[i], self.alpha[j]\n",
        "\n",
        "                    # Compute L and H\n",
        "                    if y[i] != y[j]:\n",
        "                        L = max(0, self.alpha[j] - self.alpha[i])\n",
        "                        H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n",
        "                    else:\n",
        "                        L = max(0, self.alpha[i] + self.alpha[j] - self.C)\n",
        "                        H = min(self.C, self.alpha[i] + self.alpha[j])\n",
        "\n",
        "                    if L == H:\n",
        "                        continue\n",
        "\n",
        "                    # Compute eta\n",
        "                    eta = 2 * K[i, j] - K[i, i] - K[j, j]\n",
        "                    if eta >= 0:\n",
        "                        continue\n",
        "\n",
        "                    # Update alpha[j]\n",
        "                    self.alpha[j] -= y[j] * (Ei - Ej) / eta\n",
        "                    self.alpha[j] = np.clip(self.alpha[j], L, H)\n",
        "\n",
        "                    if abs(self.alpha[j] - alpha_j_old) < 1e-5:\n",
        "                        continue\n",
        "\n",
        "                    # Update alpha[i]\n",
        "                    self.alpha[i] += y[i] * y[j] * (alpha_j_old - self.alpha[j])\n",
        "\n",
        "                    # Compute b values\n",
        "                    b1 = self.b - Ei - y[i] * (self.alpha[i] - alpha_i_old) * K[i, i] - y[j] * (self.alpha[j] - alpha_j_old) * K[i, j]\n",
        "                    b2 = self.b - Ej - y[i] * (self.alpha[i] - alpha_i_old) * K[i, j] - y[j] * (self.alpha[j] - alpha_j_old) * K[j, j]\n",
        "\n",
        "                    if 0 < self.alpha[i] < self.C:\n",
        "                        self.b = b1\n",
        "                    elif 0 < self.alpha[j] < self.C:\n",
        "                        self.b = b2\n",
        "                    else:\n",
        "                        self.b = (b1 + b2) / 2\n",
        "\n",
        "            # Convergence check\n",
        "            diff = np.linalg.norm(self.alpha - alpha_prev)\n",
        "            if diff < self.tol:\n",
        "                break\n",
        "\n",
        "        # Compute final weight vector for linear kernel\n",
        "        if self.kernel.__name__ == '<lambda>':\n",
        "            self.w = np.sum((self.alpha * y).reshape(-1, 1) * X, axis=0)\n",
        "\n",
        "        # Support vectors\n",
        "        self.support_vectors = X[self.alpha > 0]\n",
        "\n",
        "    def _compute_error(self, X, y, K, i):\n",
        "        return np.sum(self.alpha * y * K[:, i]) + self.b - y[i]\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.kernel.__name__ == '<lambda>':  # Linear kernel\n",
        "            return np.sign(np.dot(X, self.w) + self.b)\n",
        "        else:\n",
        "            predictions = np.zeros(X.shape[0])\n",
        "            for i in range(X.shape[0]):\n",
        "                predictions[i] = np.sign(np.sum(self.alpha * y * self.kernel(X[i], self.support_vectors)) + self.b)\n",
        "            return predictions\n"
      ],
      "metadata": {
        "id": "sIJFd8krRExS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "y = np.where(y == 0, -1, 1)  # Convert labels to {-1,1}\n",
        "\n",
        "# Split into train (70%), validation (15%), and test (15%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# ----------- LDA Implementation -----------\n",
        "def compute_class_means(X, y):\n",
        "    class_labels = np.unique(y)\n",
        "    means = {label: np.mean(X[y == label], axis=0) for label in class_labels}\n",
        "    return means\n",
        "\n",
        "def compute_within_class_scatter(X, y, means):\n",
        "    n_features = X.shape[1]\n",
        "    Sw = np.zeros((n_features, n_features))\n",
        "    for label, mean in means.items():\n",
        "        class_scatter = np.dot((X[y == label] - mean).T, (X[y == label] - mean))\n",
        "        Sw += class_scatter\n",
        "    return Sw\n",
        "\n",
        "def compute_between_class_scatter(X, y, means):\n",
        "    overall_mean = np.mean(X, axis=0)\n",
        "    n_features = X.shape[1]\n",
        "    Sb = np.zeros((n_features, n_features))\n",
        "    for label, mean in means.items():\n",
        "        n_samples = X[y == label].shape[0]\n",
        "        mean_diff = (mean - overall_mean).reshape(-1, 1)\n",
        "        Sb += n_samples * np.dot(mean_diff, mean_diff.T)\n",
        "    return Sb\n",
        "\n",
        "def lda(X, y, num_components=1):\n",
        "    means = compute_class_means(X, y)\n",
        "    Sw = compute_within_class_scatter(X, y, means)\n",
        "    Sb = compute_between_class_scatter(X, y, means)\n",
        "\n",
        "    eigvals, eigvecs = np.linalg.eig(np.linalg.pinv(Sw).dot(Sb))\n",
        "    eigvals = eigvals.real  # Keep real parts\n",
        "    eigvecs = eigvecs.real  # Keep real parts\n",
        "\n",
        "    sorted_indices = np.argsort(eigvals)[::-1]\n",
        "    eigvecs = eigvecs[:, sorted_indices]\n",
        "    return eigvecs[:, :num_components]\n",
        "\n",
        "def project_data(X, eigenvectors):\n",
        "    return np.dot(X, eigenvectors)\n",
        "\n",
        "# Apply LDA\n",
        "lda_vectors = lda(X_train, y_train, num_components=1)\n",
        "X_train_lda = project_data(X_train, lda_vectors)\n",
        "X_val_lda = project_data(X_val, lda_vectors)\n",
        "X_test_lda = project_data(X_test, lda_vectors)\n",
        "\n",
        "# ----------- Train SVM from Scratch -----------\n",
        "class SVM:\n",
        "    def __init__(self, C=1.0, lr=0.001, epochs=1000):\n",
        "        self.C = C\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i, x_i in enumerate(X):\n",
        "                if y[i] * (np.dot(x_i, self.w) + self.b) < 1:\n",
        "                    self.w += self.lr * (y[i] * x_i + (-2 * self.C * self.w))\n",
        "                    self.b += self.lr * y[i]\n",
        "                else:\n",
        "                    self.w += self.lr * (-2 * self.C * self.w)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)\n",
        "\n",
        "# Train SVM model\n",
        "svm = SVM(C=1.0, lr=0.001, epochs=1000)\n",
        "svm.fit(X_train_lda, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = svm.predict(X_train_lda)\n",
        "y_val_pred = svm.predict(X_val_lda)\n",
        "y_test_pred = svm.predict(X_test_lda)\n",
        "\n",
        "# Compute accuracy\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc * 100:.2f}%\")\n",
        "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EF3p5qvSsvO",
        "outputId": "cfef3c67-d0cc-480a-cd1a-7518ca8e566f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 91.43%\n",
            "Validation Accuracy: 97.78%\n",
            "Test Accuracy: 95.56%\n"
          ]
        }
      ]
    }
  ]
}